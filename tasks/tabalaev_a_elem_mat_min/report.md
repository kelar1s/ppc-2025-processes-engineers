# Минимальное значение элементов матрицы

- Студент: Табалаев Антон Максимович
- Технология: SEQ | MPI
- Вариант: 14

## 1. Введение 

Поиск минимального элемента матрицы является одной из базовых операций линейной алгебры и важным компонентом во множестве алгоритмов обработки данных, компьютерной графики и др.
При увеличении размеров матриц последовательный алгоритм перестаёт обеспечивать требуемую производительность. Поэтому в данной работе исследуется ускорение, достигаемое за счёт параллельной обработки данных с использованием технологии MPI.

## 2. Постановка задачи

**Цель работы:**
Реализовать последовательную и MPI-параллельную версии алгоритма поиска минимального элемента матрицы, провести их сравнение и анализ эффективности.

**Определение задачи:**
Для матрицы `A` размера `m × n` необходимо определить значение: `min(A) = min(A[i][j])`, где `0 <= i < n`, `0<= j < m`.

**Ограничения:**
- Матрица должна быть представлена как вектор целых чисел.
- Размеры матрицы могут быть большими.
- Корректность должна сохраняться при любых значениях элементов.
- Результаты SEQ и MPI версий должны совпадать.

## 3. Алгоритм(Последовательная версия)

**Входные данные:** количество строк `rows`, количество колонок `columns`, вектор размера `rows * columns`.

**Выходные данные:** целое число - минимальный элемент матрицы.

**Алгоритм**:
1. Получить входной вектор `matrix`.
2. Присвоить переменной `minik` значение первого элемента вектора.
3. В цикле пройтись по всем оставшимся элементам, начиная со второго:
 - Записать в `minik` минимальное значение из текущего элемента и `minik`.
4. Вернуть значение `minik`.

**Сложность**: O(n), где `n` - размер вектора `matrix`.

### Код последовательной версии алгоритма
```
bool TabalaevAElemMatMinSEQ::RunImpl() {
  auto &matrix = std::get<2>(GetInput());

  int minik = matrix[0];
  for (size_t i = 1; i < matrix.size(); i++) {
    minik = std::min(minik, matrix[i]);
  }

  GetOutput() = minik;
  return true;
}
```

## 4. Схема распараллеливания

Алгоритм параллельно вычисляет минимальный элемент матрицы, распределяя обработку блоков данных между процессами по строкам и объединяя локальные минимумы через коллективную операцию MPI.

- **Инициализация:** Все процессы запускаются в коммуникаторе MPI_COMM_WORLD, определяется общее количество процессов и ранг текущего процесса.
- **Распределение данных:** Для каждого процесса вычисляются собственные `start` и `finish`. Если количество строк нацело делится на количество процессов то каждый получает одинаковое количество элементов, иначе остаточные строки равномерно распределяются между первыми процессами.
- **Локальные вычисления:** Каждый процесс находит минимальный элемент в своем блоке строк согласно `start` и `finish`.
- **Формирование результатов:** Локальные минимумы со всех процессов объединяются с помощью операции MPI_Allreduce с операцией MPI_MIN.
- **Формирование итога:** Глобальный минимум становится доступен на всех процессах одновременно.

### Код параллельной версии алгоритма
```
bool TabalaevAElemMatMinMPI::RunImpl() {
  auto &input = GetInput();
  auto &rows = std::get<0>(input);
  auto &columns = std::get<1>(input);
  auto &matrix = std::get<2>(input);

  int world_size = 1;
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);
  int world_rank = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

  size_t part_size = rows / world_size;
  size_t remainder = rows % world_size;

  size_t start = (world_rank * part_size) + std::min(static_cast<size_t>(world_rank), remainder);
  size_t finish = start + part_size;
  if (std::cmp_less(world_rank, remainder)) {
    finish += 1;
  }

  int local_minik = INT_MAX;

  for (size_t i = start; i < finish && i < rows; ++i) {
    for (size_t j = 0; j < columns; ++j) {
      local_minik = std::min(local_minik, matrix[(i * columns) + j]);
    }
  }

  int global_minik = 0;
  MPI_Allreduce(&local_minik, &global_minik, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);

  GetOutput() = global_minik;

  return true;
}
```

## 5. Эксперементальные исследования

### Окружение

| Параметр           | Значение                      |
|--------------------|-------------------------------|
| **OS** | Windows 11 Pro 25H2  |
| **CPU** | AMD Ryzen 5 5600X (6 cores, 12 threads, 3.70 GHz) |
| **RAM** | 16 GB DDR4 3400 МГц      |
| **Компилятор**      | MSVC 14.43.34808 |
| **Версия MPI**      | Microsoft MPI 10.1.12498.52 |

### Тестовые данные

1. Функциональные тесты:
- Используют заранее подготовленные матрицы размеров от 3×3 до 10×10 с известным минимальным элементом, размещенным в центре матрицы.
- Значения элементов матрицы генерируются случайным образом в диапазоне от минимального элемента до 250.
2. Тесты производительности:
- Используют матрицу 10×10 с элементами, вычисляемыми по формуле `i * i + j`.
- Минимальный элемент имеет фиксированное значение `-100` и размещается в центре матрицы.

### Сравнение производительности

Для сравнения производительности использовалась матрица размером `5000×5000`.

Вычисления производились по следующим формулам:

- `Ускорение = Время_последовательное / Время_параллельное`
- `Эффективность = (Ускорение / Количество_процессов) × 100%`

| Режим выполнения | Количество процессов | Время выполнения (сек) | Ускорение | Эффективность |
|------------------|---------------------|------------------------|-----------|---------------|
| **Последовательный** | | | | |
| pipeline | 1 | 0.0113237600 | 1.00x | - |
| task_run | 1 | 0.0113915400 | 1.00x | - |
| **MPI (2 процесса)** | | | | |
| pipeline | 2 | 0.0065584400 | 1.73x | 86% |
| task_run | 2 | 0.0063198800 | 1.80x | 90% |
| **MPI (4 процесса)** | | | | |
| pipeline | 4 | 0.0048238400 | 2.35x | 59% |
| task_run | 4 | 0.0051497400 | 2.21x | 55% |
| **MPI (6 процессов)** | | | | |
| pipeline | 6 | 0.0048615800 | 2.33x | 39% |
| task_run | 6 | 0.0042005600 | 2.71x | 45% |

## 6. Результаты

1. Результаты функционального тестирования
- Все функциональные тесты успешно пройдены.
- Все реализации (SEQ и MPI) работают правильно и выдают идентичные результаты.

2. Результаты сравнения производительности
- Параллельная версия с использованием MPI показывает существенное ускорение по сравнению с последовательной.
- Максимальная эффективность достигается при использовании 2 процессов (86–90%).
- При увеличении числа процессов ускорение продолжает расти, однако наблюдается снижение эффективности из-за роста накладных расходов на коммуникацию. Несмотря на это, параллельный вариант стабильно превосходит по скорости последовательный во всех измеренных режимах.

## 7. Выводы

1. Разработанные алгоритмы корректно решают поставленную задачу, что подтверждается успешным прохождением всех функциональных тестов.
2. Применение параллелизма с использованием MPI позволяет значительно ускорить выполнение программы по сравнению с последовательной реализацией.
3. Наилучшая эффективность достигается при работе на двух процессах, где накладные расходы на синхронизацию и передачу данных минимальны.
4. При увеличении числа процессов ускорение продолжает расти, однако эффективность снижается из-за увеличения коммуникационных издержек между процессами.

## 8. Источники

1. Сысоев А. В. Лекции по курсу «Параллельное программирование для кластерных систем».
2. Официальная документация Microsoft MPI — https://learn.microsoft.com/ru-ru/message-passing-interface
3. Документация Open MPI — https://www.open-mpi.org/doc/
4. C++ Reference — https://en.cppreference.com
