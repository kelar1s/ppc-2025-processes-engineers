# Умножение плотных матриц. Элементы типа double. Блочная схема, алгоритм Кэннона.

- Студент: Табалаев Антон Максимович
- Технология: SEQ | MPI
- Вариант: 1

## 1. Введение 

Умножение матриц — одна из самых ресурсоемких и часто используемых операций в высокопроизводительных вычислениях. Эффективная реализация этой операции критически важна для задач моделирования физических процессов, машинного обучения, линейной алгебры и др. В данной работе рассматривается алгоритм Кэннона — блочный алгоритм умножения матриц, специально разработанный для распределенных систем с топологией «решетка».

## 2. Постановка задачи

**Цель работы:**
Реализовать последовательную и параллельную (с использованием алгоритма Кэннона) версии умножения плотных матриц, провести их сравнение и анализ эффективности.

**Определение задачи:**
Даны две квадратные матрицы `A` и `B` размера `n x n`. Необходимо вычислить результирующую матрицу `C = A × B`.

**Ограничения:**
- Матрицы должны быть квадратными одинакового размера.
- Матрицы представлены одномерными векторами типа double.
- Алгоритм Кэннона требует, чтобы количество процессов `p` было полным квадратом `(q × q = p)`, а размер матрицы `n` делился на `q` без остатка..

## 3. Алгоритм(Последовательная версия)

**Входные данные:** размер матриц `n`, вектора `a` и `b`.

**Выходные данные:** вектор `c` размера `n * n`.

**Алгоритм:**
1. Получить входные данные.
2. Инициализировать результирующий вектор нулями.
3. Использовать классический алгоритм с тройным вложенным циклом:
 - Внешние циклы i и j перебирают строки и столбцы.
 - Внутренний цикл k вычисляет скалярное произведение строки A на столбец B.
4. Вернуть вектор c.

**Сложность:** O(n^3).

### Код последовательной версии алгоритма

```
bool TabalaevACannonMatMulSEQ::RunImpl() {
  const auto n = std::get<0>(GetInput());
  const auto &a = std::get<1>(GetInput());
  const auto &b = std::get<2>(GetInput());
  std::vector<double> c(n * n, 0.0);

  for (size_t i = 0; i < n; ++i) {
    for (size_t j = 0; j < n; ++j) {
      double sum = 0.0;
      for (size_t k = 0; k < n; ++k) {
        sum += a[(i * n) + k] * b[(k * n) + j];
      }
      c[(i * n) + j] = sum;
    }
  }

  GetOutput() = c;
  return true;
}
```

## 4. Схема распараллеливания

Алгоритм Кэннона основан на блочном разбиении матриц и их циклическом сдвиге между процессами, организованными в двумерную декартову решетку.

1. **Инициализация и проверка:**
 - Определяется размер решетки `q = sqrt(количество_процессов)`.
 - Если количество процессов не является полным квадратом или размер матрицы `n` не делится на `q` без остатка, то нулевой процесс считает результат последовательно и рассылает его через MPI_Bcast.
2. **Топология:** Для процессов создаётся двумерный декартов коммуникатор `grid_comm` с периодическими границами с помощью `MPI_Cart_create`. Каждый процесс получает свои координаты `(row col)` в решётке, которые определяют его положение и направление обмена данными.
3. **Распределение данных:** 
 - Исходные матрицы разбиваются на блоки размера `block_size x block_size`, где `block_size = n / q`.
 - Для корректного распределения подматриц используются пользовательские типы данных MPI (MPI_Type_vector и MPI_Type_create_resized):
   - `block_type` описывает один двумерный блок матрицы размером `block_size x block_size`. Он задаётся как `block_size` строк по `block_size` элементов. При этом шаг между строками равен `n` (как в исходной матрице).
   - `resized_block_type` создаётся на основе `block_type` и используется для корректировки размера типа в памяти.  Это необходимо, потому что стандартный тип, созданный с помощью `MPI_Type_vector`, включает промежутки между строками исходной матрицы. Корректировка размера, равного размеру одного элемента `double`, позволяет правильно указывать смещения `displs` и обеспечивает правильную работу распределения и сбора данных.
 - Нулевой процесс формирует вектора `sendcounts` и `displs`, а затем распределяет соответствующие блоки матриц `A` и `B` между другими процессами при помощи `MPI_Scatterv`.
4. **Начальное выравнивание блоков:** Выполняется начальный циклический сдвиг:
 - Блоки матрицы `A` сдвигаются влево на количество позиций, равное номеру строки процесса;
 - Блоки матрицы `B` сдвигаются вверх на количество позиций, равное номеру столбца процесса.
 - Сдвиги выполняются с использованием функции `MPI_Sendrecv_replace`.
5. **Основной цикл вычислений:** Алгоритм выполняется за q итераций. На каждой итерации:
 - Производится локальное умножение текущих блоков `A` и `B`;
 - Полученный результат накапливается в локальной матрице `C`;
 - Циклический сдвиг блоков `A` влево на 1 шаг, блоков `B` вверх на 1 шаг. Для обмена данными используется `MPI_Sendrecv_replace`.
6. **Сбор результатов:** После завершения вычислений нулевой процесс собирает локальные блоки матриц через `MPI_Gatherv`, а затем рассылает итоговую матрицу всем процессам через `MPI_Bcast`.
7. **Завершение работы:** Освобождаются пользовательские типы и декартов коммуникатор.

Код параллельной версии алгоритма представлен в приложении.

## 5. Экспериментальные исследования

### Окружение

| Параметр           | Значение                      |
|--------------------|-------------------------------|
| **OS** | Windows 11 Pro 25H2  |
| **CPU** | AMD Ryzen 5 5600X (6 cores, 12 threads, 3.70 GHz) |
| **RAM** | 16 GB DDR4 3400 МГц      |
| **Компилятор**      | MSVC 14.43.34808 |
| **Версия MPI**      | Microsoft MPI 10.1.12498.52 |

### Тестовые данные

1. Функциональные тесты:
- Используются заранее подготовленные квадратные матрицы размеров от 2x2 до 12x12.
- Элементы матриц генерируются по определённым формулам.
- Ожидаемый результат умножения вычисляется по последовательной формуле.
- Cравнения результатов используется допустимая погрешность `epsilon = 1e-7`, что позволяет корректно проверять точность реализации.

2. Тесты производительности:
- Размер начальной матрицы: 504 × 504 (для удобства разбиения). Если тесты запускаются в MPI, то размер матрицы корректируется так, чтобы он был кратен `q = sqrt(колиество_процессов)`. Это обеспечивает равномерное распределение блоков матрицы между процессами и корректную работу алгоритма Кэннона.
- Ожидаемый результат умножения вычисляется по последовательной формуле.
- Cравнения результатов используется допустимая погрешность `epsilon = 1e-7`, что позволяет корректно проверять точность реализации.

### Сравнение производительности



## 6. Результаты



## 7. Выводы



## 8. Источники

1. Сысоев А. В. Лекции по курсу «Параллельное программирование для кластерных систем».
2. Официальная документация Microsoft MPI — https://learn.microsoft.com/ru-ru/message-passing-interface
3. Документация Open MPI — https://www.open-mpi.org/doc/
4. C++ Reference — https://en.cppreference.com
5. Интуит — https://intuit.ru/studies/courses/1156/190/lecture/4954?page=5
6. MPIMatr — https://edu.mmcs.sfedu.ru/file.php/74/MPIMatr.pdf
7. Нижегородский государственный университет. HPCC-ресурсы — http://www.hpcc.unn.ru/?dir=883